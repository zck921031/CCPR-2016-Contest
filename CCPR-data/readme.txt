CHEAVD is a Chinese natural emotional audio-visual database published by the National Laboratory of Pattern Recognition Institute of Automation at Chinese Academy of Sciences. The corpus contains 141 minutes spontaneous emotional segments extracted from 238 speakers from films, TV plays and talk shows. The number of the speakers of the corpus makes this database a valuable addition to the existing emotional databases. In contrast to other available emotional databases, we provided multi-emotion labels and fake/suppressed emotion labels. To our best knowledge, this database is the first large-scale Chinese natural emotion corpus dealing with multi-modal and natural emotion, which makes spontaneous, multi-modal emotional research possible.

Since Chinese Character is not easy to process in some toolkits, we convert all the Chinese names of the movies and TV series, programs into digitals. The name mapping can be found in NameMapping.xlsx.

CONTENT
-----------
|avi
|wav


avi: visual signals including various emotions.
wav: speech signals corresponding to the visual signals.

In some cases, the directory is empty. Because we divide the whole corpus into training, validation and test corpora. To keep the directory structure the same, we do not delete the empty directories. 

Face Image
  The face image is extracted by IntraFace toolkit[1], where the OpevCV's Viola & Jones face detector is applied for face detection and initialisation of the Intraface tracking library. The facial landmarks generated by Intraface are used for face aligning. The face size is set to 100x100. 
  In the face image folder, the 49 landmarks' locations returned by IntraFace toolkit are saved in the text file. 
  When the face extraction fails, a blank image is saved and the locations of the corresponding landmarks are set to zero in the text file.
  
Feature File
  The audio features are extracted by openSMILE with the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) (eGeMAPSv01a.conf). Saved in train_audio.arff, val_audio.arff and test_audio.arff.
  The visual features are the LBP-TOP features extracted from non-overlapping spatial 2X2 blocks. The toolkit from [2] is utilized. Saved in train_video.arff, val_video.arff and test_video.arff.
  
Label File
  In each line, the first is the relative path and name of audio-visual segment, and the second is the emotion label.   


[1] X. Xiong and F. Dela Torre. Supervised descent method and its applications to face alignment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 532-539,2013.
[2] Guoying Zhao, Matti Pietikainen, "Dynamic texture recognition using local binary patterns with an application to facial expressions," IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29(6):915-928.